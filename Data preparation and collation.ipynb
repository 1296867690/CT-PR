{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55da5ec8-fa30-4b8b-bad3-f7a75c456259",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import community as community_louvain\n",
    "\n",
    "# 读取数据\n",
    "edges_df = pd.read_csv('statistics_edges.csv')\n",
    "\n",
    "# 构建无向加权图\n",
    "G = nx.Graph()\n",
    "for idx, row in edges_df.iterrows():\n",
    "    G.add_edge(row['Source'], row['Target'], weight=row['Weight'])\n",
    "\n",
    "# 使用 Louvain 算法进行社区检测\n",
    "partition = community_louvain.best_partition(G)\n",
    "\n",
    "# 计算每个社团的子图\n",
    "def calculate_local_structural_entropy(subgraph):\n",
    "    entropy_dict = {}\n",
    "    \n",
    "    for node in subgraph.nodes():\n",
    "        neighbors = list(subgraph.neighbors(node))\n",
    "        if len(neighbors) == 0:\n",
    "            entropy_dict[node] = 0\n",
    "            continue\n",
    "        \n",
    "        degrees = [subgraph.degree(neighbor) for neighbor in neighbors]\n",
    "        degree_counts = np.bincount(degrees)\n",
    "        degree_prob = degree_counts / np.sum(degree_counts)\n",
    "        \n",
    "        # 防止对数计算中的零值\n",
    "        entropy = -np.sum(degree_prob * np.log(degree_prob + np.finfo(float).eps))\n",
    "        entropy_dict[node] = entropy\n",
    "    \n",
    "    return entropy_dict\n",
    "\n",
    "# 获取每个节点所属的社区\n",
    "communities = set(partition.values())\n",
    "\n",
    "# 提取每个社团的子图并计算局部结构熵\n",
    "for community_id in communities:\n",
    "    # 获取当前社团的节点\n",
    "    nodes_in_community = [node for node, comm in partition.items() if comm == community_id]\n",
    "    subgraph = G.subgraph(nodes_in_community)\n",
    "    \n",
    "    # 计算局部结构熵\n",
    "    local_entropy = calculate_local_structural_entropy(subgraph)\n",
    "    \n",
    "    # 打印结果\n",
    "    print(f\"Community {community_id}:\")\n",
    "    for node, entropy in local_entropy.items():\n",
    "        print(f\"  Node {node}: Local Structural Entropy = {entropy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2e4bd2-e525-4e0f-b968-b4891dcb838c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 创建一个 DataFrame 来存储结果\n",
    "results = []\n",
    "\n",
    "# 提取每个社团的子图并计算局部结构熵\n",
    "for community_id in communities:\n",
    "    # 获取当前社团的节点\n",
    "    nodes_in_community = [node for node, comm in partition.items() if comm == community_id]\n",
    "    subgraph = G.subgraph(nodes_in_community)\n",
    "    \n",
    "    # 计算局部结构熵\n",
    "    local_entropy = calculate_local_structural_entropy(subgraph)\n",
    "    \n",
    "    # 收集结果\n",
    "    for node, entropy in local_entropy.items():\n",
    "        results.append({'Node': node, 'Local Structural Entropy': entropy})\n",
    "\n",
    "# 将结果保存为 CSV 文件\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('local_structural_entropy1.csv', index=False)\n",
    "\n",
    "print(\"Results have been saved to 'local_structural_entropy.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782cc4ac-4d31-42d4-8324-89be8507c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建社区图\n",
    "community_graph = nx.Graph()\n",
    "\n",
    "# 遍历原图中的边，根据社区进行合并\n",
    "for u, v, data in G.edges(data=True):\n",
    "    community_u = partition[u]\n",
    "    community_v = partition[v]\n",
    "    \n",
    "    # 如果两个节点属于不同社区，则在社区图中添加边\n",
    "    if community_u != community_v:\n",
    "        if community_graph.has_edge(community_u, community_v):\n",
    "            # 如果边已经存在，则累加权重\n",
    "            community_graph[community_u][community_v]['weight'] += data['weight']\n",
    "        else:\n",
    "            # 否则添加新边\n",
    "            community_graph.add_edge(community_u, community_v, weight=data['weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a7d65e-cd42-41c2-8076-b91ae58a7c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将社区图的边数据保存为 CSV\n",
    "community_edges = nx.to_pandas_edgelist(community_graph)\n",
    "community_edges.to_csv('community_edges.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2a5843-9ad3-463b-babe-837f261eba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算度中心性\n",
    "degree_centrality = nx.degree_centrality(community_graph)\n",
    "\n",
    "# 计算接近中心性\n",
    "closeness_centrality = nx.closeness_centrality(community_graph)\n",
    "\n",
    "# 计算介数中心性\n",
    "#betweenness_centrality = nx.betweenness_centrality(community_graph)\n",
    "\n",
    "# 计算特征向量中心性\n",
    "eigenvector_centrality = nx.eigenvector_centrality(community_graph)\n",
    "\n",
    "# 计算负载中心性\n",
    "#load_centrality = nx.load_centrality(community_graph)\n",
    "\n",
    "# 计算信息中心性\n",
    "information_centrality = nx.information_centrality(community_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d015a-2b95-49b8-8d18-5f5507e22a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 创建 DataFrame 来存储各类中心性指标\n",
    "centralities_df = pd.DataFrame({\n",
    "    'Degree Centrality': degree_centrality,\n",
    "    'Closeness Centrality': closeness_centrality,\n",
    "    #'Betweenness Centrality': betweenness_centrality,\n",
    "    'Eigenvector Centrality': eigenvector_centrality,\n",
    "    #'Load Centrality': load_centrality,\n",
    "    'Information Centrality': information_centrality\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1c8d96-57ab-4345-a0a6-f4bf64057b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据标准化 (0, 1) 范围\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "normalized_matrix = scaler.fit_transform(centralities_df)\n",
    "normalized_df = pd.DataFrame(normalized_matrix, columns=centralities_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fb1c90-eda5-4e4e-9c85-6d05e23282a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 计算每个指标的熵\n",
    "def calculate_entropy(normalized_matrix):\n",
    "    epsilon = 1e-10  # 防止log(0)\n",
    "    P = normalized_matrix / normalized_matrix.sum(axis=0)  # 归一化\n",
    "    log_P = np.log(P + epsilon)\n",
    "    entropy = -np.sum(P * log_P, axis=0) / np.log(len(P))\n",
    "    return entropy\n",
    "\n",
    "# 计算熵权\n",
    "entropy = calculate_entropy(normalized_matrix)\n",
    "weights = (1 - entropy) / np.sum(1 - entropy)  # 根据熵值计算权重\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3656cc9a-8b15-4ae8-b8f3-724798b48cf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 正理想解和负理想解\n",
    "positive_ideal_solution = normalized_matrix.max(axis=0)\n",
    "negative_ideal_solution = normalized_matrix.min(axis=0)\n",
    "# 计算到正理想解和负理想解的距离\n",
    "positive_distance = np.sqrt(np.sum((normalized_matrix - positive_ideal_solution) ** 2 * weights, axis=1))\n",
    "negative_distance = np.sqrt(np.sum((normalized_matrix - negative_ideal_solution) ** 2 * weights, axis=1))\n",
    "# 计算综合评价分数\n",
    "topsis_score = negative_distance / (positive_distance + negative_distance)\n",
    "\n",
    "# 将结果添加到 DataFrame\n",
    "centralities_df['TOPSIS Score'] = topsis_score\n",
    " 保存为 CSV 文件\n",
    "centralities_df.to_csv('community_centralities_with_topsis.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c420336b-e39c-490d-9b5a-0aa758688205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从 partition 获取所有唯一的社区编号\n",
    "partition_communities = set(partition.values())\n",
    "print(\"Partition 中的社区编号:\", partition_communities)\n",
    "# 获取 centralities_df 中的社区编号\n",
    "centralities_communities = set(centralities_df.index)\n",
    "print(\"centralities_df 中的社区编号:\", centralities_communities)\n",
    "# 找出 partition 中有但 centralities_df 中没有的社区编号\n",
    "missing_in_centralities = partition_communities - centralities_communities\n",
    "print(\"在 partition 中但缺失于 centralities_df 的社区编号:\", missing_in_centralities)\n",
    "\n",
    "# 找出 centralities_df 中有但 partition 中没有的社区编号\n",
    "extra_in_centralities = centralities_communities - partition_communities\n",
    "print(\"在 centralities_df 中但不存在于 partition 的社区编号:\", extra_in_centralities)\n",
    "#查看缺失社区中的节点\n",
    "missing_communities =  {24} # 使用你检测到的缺失社区编号\n",
    "missing_nodes = [node for node, community in partition.items() if community in missing_communities]\n",
    "print(\"缺失社区中的节点:\", missing_nodes)\n",
    "\n",
    "# 检查这些节点的连通性和边\n",
    "for node in missing_nodes:\n",
    "    print(f\"节点 {node} 的邻居:\", list(G.neighbors(node)))\n",
    "# 为缺失社区分配默认值 (如 0)\n",
    "for missing_community in missing_communities:\n",
    "    community_topsis_scores[missing_community] = 0  # 或 np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e70df-e9b9-481d-93e7-8536e227e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为这些缺失社区创建一个 DataFrame，默认 TOPSIS 得分为 0（或 np.nan）\n",
    "missing_communities_df = pd.DataFrame({\n",
    "    'Community': list(missing_communities),\n",
    "    'TOPSIS Score': 0  # 或 np.nan\n",
    "})\n",
    "\n",
    "# 如果 centralities_df 没有社区编号列，确保它的索引是社区编号\n",
    "centralities_df = centralities_df.reset_index().rename(columns={'index': 'Community'})\n",
    "\n",
    "# 合并缺失社区到 centralities_df 中\n",
    "centralities_df = pd.concat([centralities_df, missing_communities_df], ignore_index=True)\n",
    "\n",
    "# 确保 Community 列是索引\n",
    "centralities_df.set_index('Community', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c0d73-0445-4135-b0fd-6c44adb9f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从 partition 获取每个节点所属的社区编号\n",
    "node_community_mapping = partition  # 这是一个 {node_id: community_id} 的字典\n",
    "\n",
    "# 获取社区的 TOPSIS 得分\n",
    "community_topsis_scores = centralities_df['TOPSIS Score'].to_dict()  # {community_id: topsis_score}\n",
    "\n",
    "# 为每个原始节点分配其所属社区的 TOPSIS 得分\n",
    "node_topsis_scores = {node: community_topsis_scores[community] for node, community in node_community_mapping.items()}\n",
    "\n",
    "# 将结果转化为 DataFrame\n",
    "node_topsis_df = pd.DataFrame(list(node_topsis_scores.items()), columns=['Node', 'TOPSIS Score'])\n",
    "\n",
    "# 保存结果为 CSV\n",
    "node_topsis_df.to_csv('node_topsis_scores1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf6b0c-9dd8-4036-a3ef-163ce3f10850",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取 CSV 文件\n",
    "topsis_df = pd.read_csv('node_topsis_scores1.csv')\n",
    "entropy_df = pd.read_csv('local_structural_entropy1.csv')\n",
    "\n",
    "# 打印前几行数据以检查\n",
    "print(\"TOPSIS Data:\")\n",
    "print(topsis_df.head())\n",
    "print(\"\\nLocal Structural Entropy Data:\")\n",
    "print(entropy_df.head())\n",
    "\n",
    "# 合并两个 DataFrame，根据 'Node' 列\n",
    "merged_df = pd.merge(topsis_df, entropy_df, on='Node')\n",
    "\n",
    "# 打印合并后的数据以检查\n",
    "print(\"\\nMerged Data:\")\n",
    "print(merged_df.head())\n",
    "\n",
    "# 保存合并后的结果到新的 CSV 文件\n",
    "merged_df.to_csv('s_node_data1.csv', index=False)\n",
    "\n",
    "print(\"Merged data has been saved to 'merged_node_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb2a66-a2f3-45a6-ae32-a8f0b14cc5eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取 CSV 文件\n",
    "topsis_df = pd.read_csv('node_topsis_scores1.csv')\n",
    "entropy_df = pd.read_csv('local_structural_entropy1.csv')\n",
    "\n",
    "# 合并两个 DataFrame，根据 'Node' 列\n",
    "merged_df = pd.merge(topsis_df, entropy_df, on='Node')\n",
    "\n",
    "\n",
    "# 计算 e^TOPSIS Score * Local Structural Entropy\n",
    "merged_df['Final_Score'] = (merged_df['TOPSIS Score'] + 1)* merged_df['Local Structural Entropy']\n",
    "\n",
    "# 按 Final_Score 降序排序\n",
    "sorted_df = merged_df.sort_values(by='Final_Score', ascending=False)\n",
    "\n",
    "# 打印排序后的数据以检查\n",
    "print(\"Sorted Data:\")\n",
    "print(sorted_df.head())\n",
    "\n",
    "# 保存排序后的结果到新的 CSV 文件\n",
    "sorted_df.to_csv('s_final_scores1.csv', index=False)\n",
    "\n",
    "print(\"Sorted final scores have been saved to 'sorted_final_scores.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6edb7d0-41bf-4487-9380-764c971ef934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 将 Final_Score 作为初始的 PageRank 权重\n",
    "initial_pagerank = merged_df.set_index('Node')['Final_Score'].to_dict()\n",
    "\n",
    "# 计算 PageRank\n",
    "pagerank_scores = nx.pagerank(G, personalization=initial_pagerank)\n",
    "\n",
    "\n",
    "# 将 PageRank 结果保存到 DataFrame\n",
    "pagerank_df = pd.DataFrame(list(pagerank_scores.items()), columns=['Node', 'PageRank'])\n",
    "\n",
    "# 打印 PageRank 结果以检查\n",
    "print(\"PageRank Results:\")\n",
    "print(pagerank_df.head())\n",
    "\n",
    "# 保存 PageRank 结果到新的 CSV 文件\n",
    "pagerank_df.to_csv('pagerank_results1.csv', index=False)\n",
    "\n",
    "print(\"PageRank results have been saved to 'pagerank_results.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4be764-8171-458b-a23a-c45b2d95086d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import kendalltau\n",
    "import networkx as nx\n",
    "\n",
    "# 计算各种中心性指标\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "\n",
    "# 自定义方法得分 (例如用 PageRank 计算后的分数)\n",
    "custom_scores_df = pd.read_csv('pagerank_results.csv')\n",
    "custom_centrality = custom_scores_df.set_index('Node')['PageRank'].to_dict()\n",
    "\n",
    "# 将所有中心性结果存储在一个字典中\n",
    "centrality_dict = {\n",
    "    'Degree': degree_centrality,\n",
    "    'Closeness': closeness_centrality,\n",
    "    'Betweenness': betweenness_centrality,\n",
    "    'Eigenvector': eigenvector_centrality,\n",
    "    'Custom': custom_centrality\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b7cacb-67d1-4c18-b54b-31742fa91206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# 将结果排序并存储到 CSV 文件\n",
    "for method, centrality in centrality_dict.items():\n",
    "    # 创建 DataFrame 并按中心性值排序\n",
    "    df = pd.DataFrame(list(centrality.items()), columns=['Node', 'Centrality'])\n",
    "    df = df.sort_values(by='Centrality', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # 保存为 CSV 文件\n",
    "    filename = f'{method.lower()}_sorted1.csv'\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f'Saved {method} centrality results to {filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3989b55c-c403-4a30-9b07-57a91143ac31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
